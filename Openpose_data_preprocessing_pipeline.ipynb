{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/zakmoss/OpenPose_maneuver_grader/blob/main/Openpose_data_preprocessing_pipeline.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive', force_remount=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "S98veVCBXdyc",
        "outputId": "0cb798f0-de0b-4ec5-840c-50841ff19c8a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# prev\n",
        "\n",
        "import os\n",
        "import json\n",
        "import numpy as np\n",
        "import cv2\n",
        "import os\n",
        "\n",
        "#before running script you need the jsons and the original video. place the json folder and the video within a folder in your google drive\n",
        "\n",
        "# 1. CREATE A FOLDER in google drive that contains the json folder file. also, put the original video in there too.\n",
        "# 2. DEFINE THESE VARIABLES BEFORE\n",
        "\n",
        "# parent folder name with a / at the end. for example, if named example, insert 'example/'. dont forget to add the sub-parent folder\n",
        "input_folder_name = 'Surfing with Pose Estimation/rhGeneral/example62/'\n",
        "\n",
        "#  folder with jsons and video.\n",
        "input_jsons = '62_jsons'\n",
        "input_video = '62.avi'\n",
        "\n",
        "# input the video name without .avi at the end. for example, if input_video is surfVid.avi, input_jpg will be surfVid\n",
        "input_jpg = '62'\n",
        "\n",
        "\n",
        "def json_to_npy_converter(input_folder, output_folder):\n",
        "    if not os.path.exists(output_folder):\n",
        "        os.makedirs(output_folder)\n",
        "\n",
        "    for file_name in os.listdir(input_folder):\n",
        "        if file_name.endswith(\".json\"):\n",
        "            input_file_path = os.path.join(input_folder, file_name)\n",
        "\n",
        "            with open(input_file_path, 'r') as file:\n",
        "                try:\n",
        "                    data = json.load(file)\n",
        "                    for person in data['people']:\n",
        "                        keypoints = person['pose_keypoints_2d']\n",
        "                        keypoints_array = np.array(keypoints).reshape(-1, 3)\n",
        "                        output_file_path = os.path.join(output_folder, file_name.replace('.json', '.npy'))\n",
        "                        np.save(output_file_path, keypoints_array)\n",
        "                        print(f\"Saved NPY file: {output_file_path}\")\n",
        "                except Exception as e:\n",
        "                    print(f\"Error processing file {input_file_path}: {e}\")\n",
        "\n",
        "# Replace the example paths with your actual paths\n",
        "input_folder = '/content/drive/My Drive/' + input_folder_name + input_jsons\n",
        "output_folder =  '/content/drive/My Drive/' + input_folder_name + '/output1'\n",
        "json_to_npy_converter(input_folder, output_folder)\n",
        "\n",
        "\n",
        "# video path extraction\n",
        "\n",
        "def extract_first_frame(video_path):\n",
        "    cap = cv2.VideoCapture(video_path)\n",
        "    if not cap.isOpened():\n",
        "        print(\"Error opening video file.\")\n",
        "        return\n",
        "\n",
        "    success, frame = cap.read()\n",
        "    if success:\n",
        "        output_filename = os.path.splitext(os.path.basename(video_path))[0] + \"frame.jpg\"\n",
        "        output_path = os.path.join(os.path.dirname(video_path), output_filename)\n",
        "\n",
        "        cv2.imwrite(output_path, frame)\n",
        "        print(f\"Saved first frame to {output_path}\")\n",
        "    else:\n",
        "        print(\"Failed to extract the first frame.\")\n",
        "\n",
        "    cap.release()\n",
        "#replace example path with actual path\n",
        "video_path = '/content/drive/My Drive/' + input_folder_name + input_video\n",
        "extract_first_frame(video_path)\n",
        "\n",
        "#finds frame center\n",
        "\n",
        "from PIL import Image\n",
        "\n",
        "def get_center_coordinates(image_path):\n",
        "    with Image.open(image_path) as img:\n",
        "        with Image.open(image_path):\n",
        "            height, width = img.size\n",
        "            center_x = width/2\n",
        "            center_y = height/2\n",
        "            return center_x,center_y\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "image_path = '/content/drive/My Drive/' + input_folder_name + input_jpg + 'frame.jpg'\n",
        "center_coordinates = get_center_coordinates(image_path)\n",
        "print(f\"{center_coordinates}\")\n",
        "\n",
        "#resize pose estimates to consistent size (height = 220 pixels)\n",
        "\n",
        "import numpy as np\n",
        "import os\n",
        "\n",
        "def rescale_pose_estimates(input_folder, output_folder, target_height=220):\n",
        "\n",
        "\n",
        "    if not os.path.exists(output_folder):\n",
        "        os.makedirs(output_folder)\n",
        "\n",
        "    for file_name in os.listdir(input_folder):\n",
        "        if file_name.endswith('.npy'):\n",
        "            file_path = os.path.join(input_folder, file_name)\n",
        "            pose_estimates = np.load(file_path)\n",
        "\n",
        "            # Calculate the current height\n",
        "            max_y = np.max(pose_estimates[:, 1])\n",
        "            min_y = np.min(pose_estimates[:, 1])\n",
        "            current_height = max_y - min_y\n",
        "\n",
        "\n",
        "            scaling_factor = target_height / current_height if current_height > 0 else 0\n",
        "\n",
        "\n",
        "            pose_estimates[:, :2] *= scaling_factor\n",
        "\n",
        "\n",
        "            new_file_path = os.path.join(output_folder, file_name)\n",
        "            np.save(new_file_path, pose_estimates)\n",
        "\n",
        "#replace the example paths with actual paths\n",
        "input_folder = output_folder\n",
        "output_folder = '/content/drive/My Drive/' + input_folder_name + '/output2'\n",
        "\n",
        "\n",
        "rescale_pose_estimates(input_folder, output_folder)\n",
        "\n",
        "#center keypoints in frame\n",
        "#use output from get_center_coordinates for \"frame_center\" variable (duh)\n",
        "#input folder is the ouput folder from rescale_pose_estimate\n",
        "\n",
        "import numpy as np\n",
        "import os\n",
        "\n",
        "def center_and_save_pose_estimates(input_folder, output_folder, frame_center):\n",
        "\n",
        "    if not os.path.exists(output_folder):\n",
        "        os.makedirs(output_folder)\n",
        "\n",
        "\n",
        "    for file_name in os.listdir(input_folder):\n",
        "        if file_name.endswith('.npy'):\n",
        "            file_path = os.path.join(input_folder, file_name)\n",
        "            keypoints = np.load(file_path)\n",
        "\n",
        "\n",
        "            center_of_mass = np.mean(keypoints[:, :2], axis=0)\n",
        "\n",
        "            difference = center_of_mass-frame_center\n",
        "\n",
        "            centered_keypoints = keypoints.copy()\n",
        "\n",
        "\n",
        "            centered_keypoints[:, :2] -= difference\n",
        "\n",
        "\n",
        "            output_path = os.path.join(output_folder, file_name)\n",
        "            np.save(output_path, centered_keypoints)\n",
        "\n",
        "\n",
        "input_folder = output_folder\n",
        "output_folder = '/content/gdrive/My Drive/' + input_folder_name + '/output3'\n",
        "frame_center = center_coordinates\n",
        "\n",
        "center_and_save_pose_estimates(input_folder, output_folder, frame_center)\n",
        "\"subtract difference between reference point and center point from each coordinate\"\n",
        "#check pose heights from each frame\n",
        "\n",
        "import numpy as np\n",
        "import os\n",
        "\n",
        "def calculate_pose_heights(directory):\n",
        "\n",
        "    heights = {}\n",
        "\n",
        "    for file_name in os.listdir(directory):\n",
        "        if file_name.endswith('.npy'):\n",
        "            file_path = os.path.join(directory, file_name)\n",
        "            keypoints = np.load(file_path)\n",
        "\n",
        "            height = np.max(keypoints[:, 1]) - np.min(keypoints[:, 1])\n",
        "            heights[file_name] = height\n",
        "\n",
        "    return heights\n",
        "\n",
        "#replace the example paths with actual paths\n",
        "directory =  output_folder\n",
        "\n",
        "\n",
        "pose_heights = calculate_pose_heights(directory)\n",
        "for file_name, height in pose_heights.items():\n",
        "    print(f\"{file_name}: {height}\")\n",
        "\n",
        "    #seperate pose estimates into arrays corresponding to keypoint ordering ^\n",
        "\n",
        "import numpy as np\n",
        "import os\n",
        "\n",
        "\n",
        "def bodypart_array_maker(input_folder, output_folder):\n",
        "\n",
        "    if not os.path.exists(output_folder):\n",
        "        os.makedirs(output_folder)\n",
        "\n",
        "    keypoints_accumulator = {} # a dictionary where the key refers to the keypoint index and the value is a list containing the corresponding keypoint accross each frame\n",
        "    #dictionary contains 25 indeces and each index contains a list that is 319 long\n",
        "    for file_name in os.listdir(input_folder):\n",
        "        if file_name.endswith('.npy'):\n",
        "            file_path = os.path.join(input_folder, file_name)\n",
        "            keypoints = np.load(file_path)\n",
        "\n",
        "            for i, keypoint in enumerate(keypoints):\n",
        "                if i not in keypoints_accumulator:\n",
        "                    keypoints_accumulator[i] = []   #creates list for each keypoint\n",
        "                keypoints_accumulator[i].append(keypoint) #populates each list one by one\n",
        "\n",
        "        for keypoint_index, keypoint_list in keypoints_accumulator.items():\n",
        "            output_filepath = os.path.join(output_folder, f'keypoint_{keypoint_index}.npy')\n",
        "            np.save(output_filepath, np.array(keypoint_list))\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "input_folder = output_folder\n",
        "output_folder = '/content/drive/My Drive/' + input_folder_name + '/output4'\n",
        "bodypart_array_maker(input_folder, output_folder)\n",
        "\n",
        "\n",
        "import pandas as pd\n",
        "import os\n",
        "import numpy as np\n",
        "import scipy as sp\n",
        "from scipy.ndimage import gaussian_filter1d\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "keypoint_names = {\n",
        "    0: \"Nose\",\n",
        "    1: \"Neck\",\n",
        "    2: \"RShoulder\",\n",
        "    3: \"RElbow\",\n",
        "    4: \"RWrist\",\n",
        "    5: \"LShoulder\",\n",
        "    6: \"LElbow\",\n",
        "    7: \"LWrist\",\n",
        "    8: \"MidHip\",\n",
        "    9: \"RHip\",\n",
        "    10: \"RKnee\",\n",
        "    11: \"RAnkle\",\n",
        "    12: \"LHip\",\n",
        "    13: \"LKnee\",\n",
        "    14: \"LAnkle\",\n",
        "    15: \"REye\",\n",
        "    16: \"LEye\",\n",
        "    17: \"REar\",\n",
        "    18: \"LEar\",\n",
        "    19: \"LBigToe\",\n",
        "    20: \"LSmallToe\",\n",
        "    21: \"LHeel\",\n",
        "    22: \"RBigToe\",\n",
        "    23: \"RSmallToe\",\n",
        "    24: \"RHeel\",\n",
        "    25: \"Background\"\n",
        "}\n",
        "\n",
        "\n",
        "def smooth_data(input_folder, output_folder, plot_folder):\n",
        "    if not os.path.exists(output_folder):\n",
        "        os.makedirs(output_folder)\n",
        "    if not os.path.exists(plot_folder):\n",
        "        os.makedirs(plot_folder)\n",
        "    for index, file_name in enumerate(sorted(os.listdir(input_folder))):\n",
        "        if file_name.endswith('.npy'):\n",
        "            keypoint_name = keypoint_names.get(index, \"Unknown\")\n",
        "            plot_file_name = f\"{keypoint_name}.png\"\n",
        "            plot_path = os.path.join(plot_folder, plot_file_name)\n",
        "\n",
        "            output_npy_path = os.path.join(output_folder,keypoint_name)\n",
        "            file_path = os.path.join(input_folder, file_name)\n",
        "            pose_estimates = np.load(file_path)\n",
        "\n",
        "            pre_smoothing_x = pose_estimates[:,0].copy()\n",
        "            pre_smoothing_y = pose_estimates[:,1].copy()\n",
        "\n",
        "            #Fill in missing X coords\n",
        "            X_series = pd.Series(pose_estimates[:,0])\n",
        "            filled_series = X_series.fillna(method='ffill').fillna(0)\n",
        "            pose_estimates[:,0] = filled_series.to_numpy()\n",
        "\n",
        "            #fill in missing Y coords\n",
        "            Y_series = pd.Series(pose_estimates[:,1])\n",
        "            filled_series = Y_series.fillna(method='ffill').fillna(0)\n",
        "            pose_estimates[:,1] = filled_series.to_numpy()\n",
        "\n",
        "            #apply smoothing\n",
        "            pose_estimates[:,:] = gaussian_filter1d(pose_estimates[:,:], sigma=15, axis=0)\n",
        "            startingPt = np.nanmedian(pose_estimates[:,0:9], axis=0)\n",
        "            pose_estimates[:,:] = pose_estimates[:,:] - startingPt\n",
        "\n",
        "            post_smoothing_x = pose_estimates[:,0]\n",
        "            post_smoothing_y = pose_estimates[:,1]  # Post-smoothing Y coordinates\n",
        "\n",
        "            fig = plt.figure(figsize=(10, 7))\n",
        "            ax = fig.add_subplot(111, projection='3d')  # Create a 3D subplot\n",
        "\n",
        "            row_indices = np.arange(pose_estimates.shape[0])\n",
        "\n",
        "            # Plot pre and post smoothing data points in 3D\n",
        "            ax.scatter(pre_smoothing_x, pre_smoothing_y, row_indices, color='green', label='Pre-Smoothing')\n",
        "            ax.scatter(post_smoothing_x, post_smoothing_y, row_indices, color='red', label='Post-Smoothing')\n",
        "\n",
        "            ax.set_title(f'Coordinates Before and After Smoothing for {keypoint_name}')\n",
        "            ax.set_xlabel('X Coordinate')\n",
        "            ax.set_ylabel('Y Coordinate')\n",
        "            ax.set_zlabel('Row Index / Time')\n",
        "            ax.legend()\n",
        "\n",
        "            plt.savefig(plot_path)  # Save the 3D plot\n",
        "            plt.close()  # Close the plot figure\n",
        "            np.save(output_npy_path, pose_estimates)\n",
        "\n",
        "\n",
        "#replace the example paths with actual paths\n",
        "input_folder = output_folder\n",
        "output_folder = '/content/drive/My Drive/' + input_folder_name + '/outputBody'\n",
        "plot_folder = '/content/drive/My Drive/' + input_folder_name + '/outputPlots'\n",
        "\n",
        "smooth_data(input_folder, output_folder, plot_folder)\n",
        "print('plots printed to ' + plot_folder)\n",
        "\n",
        "#vectroize each keypoint into a massive matrix\n",
        "\n",
        "\n",
        "import numpy as np\n",
        "import os\n",
        "\n",
        "def transform_keypoints(input_folder, output_folder):\n",
        "    if not os.path.exists(output_folder):\n",
        "        os.makedirs(output_folder)\n",
        "\n",
        "    for file_name in os.listdir(input_folder):\n",
        "        if file_name.endswith('.npy'):\n",
        "            # Load the current keypoints array\n",
        "            file_path = os.path.join(input_folder, file_name)\n",
        "            keypoints = np.load(file_path)\n",
        "\n",
        "            #get rid of confidence score\n",
        "            keypoints_xy = keypoints[:, :2]\n",
        "\n",
        "\n",
        "            keypoints_vectorized = keypoints_xy.reshape(-1, 1)\n",
        "\n",
        "\n",
        "            output_file_path = os.path.join(output_folder, file_name)\n",
        "            np.save(output_file_path, keypoints_vectorized)\n",
        "\n",
        "input_folder = output_folder\n",
        "output_folder = '/content/drive/My Drive/' + input_folder_name + '/vectorized_arrays'\n",
        "#'/content/gdrive/My Drive/' + input_folder_name + '/vectorized_arrays'\n",
        "\n",
        "transform_keypoints(input_folder, output_folder)\n",
        "\n",
        "import numpy as np\n",
        "import os\n",
        "\n",
        "def fat_matrix_row_maker(input_folder, big_matrix_file_path):\n",
        "    # Initialize an empty array for the big matrix. This will hold all stacked vectors.\n",
        "    fat_matrix_row = None\n",
        "\n",
        "    for file_name in os.listdir(input_folder):\n",
        "        if file_name.endswith('.npy'):\n",
        "            # Load the vectorized keypoints file\n",
        "            file_path = os.path.join(input_folder, file_name)\n",
        "            vectorized_keypoints = np.load(file_path, allow_pickle=True)\n",
        "\n",
        "            # Check if big_matrix is not initialized\n",
        "            if fat_matrix_row is None:\n",
        "                fat_matrix_row = vectorized_keypoints\n",
        "            else:\n",
        "                # Stack horizontally (add as a new column)\n",
        "                fat_matrix_row = np.hstack((fat_matrix_row, vectorized_keypoints))\n",
        "\n",
        "    # Save the big matrix to the specified file path\n",
        "    file_name=\"fat_matrix_row\"\n",
        "    output_file_path = os.path.join(output_folder, file_name)\n",
        "    np.save(output_file_path, fat_matrix_row)\n",
        "\n",
        "# Specify the input folder containing the vectorized arrays\n",
        "input_folder = output_folder\n",
        "\n",
        "#'/content/gdrive/My Drive/<input_folder_name>/vectorized_arrays'\n",
        "\n",
        "# Specify the path where you want to save the big matrix\n",
        "output_folder = '/content/drive/My Drive/' + input_folder_name\n",
        "\n",
        "# Call the function to stack all vectorized keypoints into a big matrix\n",
        "fat_matrix_row_maker(input_folder, output_folder)\n",
        "\n",
        "import numpy as np\n",
        "import os\n",
        "\n",
        "def update_vertical_matrix(input_folder, output_file_path):\n",
        "    # Check if the output file already exists\n",
        "    if os.path.exists(output_file_path):\n",
        "        # If it exists, load the existing matrix\n",
        "        vertical_matrix = np.load(output_file_path, allow_pickle=False)\n",
        "    else:\n",
        "        # If not, initialize it as None (or an empty array)\n",
        "        vertical_matrix = None\n",
        "\n",
        "    # List all .npy files in the input_folder\n",
        "    npy_files = [f for f in os.listdir(input_folder) if f.endswith('.npy')]\n",
        "\n",
        "    # Load each .npy file and stack them vertically\n",
        "    for file_name in npy_files:\n",
        "        file_path = os.path.join(input_folder, file_name)\n",
        "        matrix_row = np.load(file_path, allow_pickle=True)\n",
        "\n",
        "\n",
        "\n",
        "        # If vertical_matrix is None, initialize it with the first matrix_row.\n",
        "        if vertical_matrix is None:\n",
        "            vertical_matrix = matrix_row\n",
        "        else:\n",
        "            # Stack it vertically\n",
        "            vertical_matrix = np.vstack((vertical_matrix, matrix_row))\n",
        "\n",
        "    # Save the updated vertically stacked matrix to the output file path\n",
        "    np.save(output_file_path, vertical_matrix)\n",
        "    print(f\"Updated matrix saved to {output_file_path}. Shape: {vertical_matrix.shape}\")\n",
        "\n",
        "# Specify the folder containing the fat matrix rows as .npy files\n",
        "input_folder = output_folder\n",
        "\n",
        "# Specify the path where you want to save the vertically stacked big matrix\n",
        "output_file_path = '/content/drive/My Drive/Surfing with Pose Estimation/vertical_stacked_matrix.npy'\n",
        "\n",
        "# Call the function to stack all fat matrix rows into one big vertically stacked matrix\n",
        "update_vertical_matrix(input_folder, output_file_path)"
      ],
      "metadata": {
        "id": "Z2xtsqO1wKNX",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "061ea935-6ee6-4611-d2bc-da5296f21fb3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saved NPY file: /content/drive/My Drive/Surfing with Pose Estimation/rhGeneral/example62//output1/62_000000000057_keypoints.npy\n",
            "Saved NPY file: /content/drive/My Drive/Surfing with Pose Estimation/rhGeneral/example62//output1/62_000000000031_keypoints.npy\n",
            "Saved NPY file: /content/drive/My Drive/Surfing with Pose Estimation/rhGeneral/example62//output1/62_000000000023_keypoints.npy\n",
            "Saved NPY file: /content/drive/My Drive/Surfing with Pose Estimation/rhGeneral/example62//output1/62_000000000048_keypoints.npy\n",
            "Saved NPY file: /content/drive/My Drive/Surfing with Pose Estimation/rhGeneral/example62//output1/62_000000000049_keypoints.npy\n",
            "Saved NPY file: /content/drive/My Drive/Surfing with Pose Estimation/rhGeneral/example62//output1/62_000000000037_keypoints.npy\n",
            "Saved NPY file: /content/drive/My Drive/Surfing with Pose Estimation/rhGeneral/example62//output1/62_000000000060_keypoints.npy\n",
            "Saved NPY file: /content/drive/My Drive/Surfing with Pose Estimation/rhGeneral/example62//output1/62_000000000029_keypoints.npy\n",
            "Saved NPY file: /content/drive/My Drive/Surfing with Pose Estimation/rhGeneral/example62//output1/62_000000000027_keypoints.npy\n",
            "Saved NPY file: /content/drive/My Drive/Surfing with Pose Estimation/rhGeneral/example62//output1/62_000000000030_keypoints.npy\n",
            "Saved NPY file: /content/drive/My Drive/Surfing with Pose Estimation/rhGeneral/example62//output1/62_000000000028_keypoints.npy\n",
            "Saved NPY file: /content/drive/My Drive/Surfing with Pose Estimation/rhGeneral/example62//output1/62_000000000025_keypoints.npy\n",
            "Saved NPY file: /content/drive/My Drive/Surfing with Pose Estimation/rhGeneral/example62//output1/62_000000000020_keypoints.npy\n",
            "Saved NPY file: /content/drive/My Drive/Surfing with Pose Estimation/rhGeneral/example62//output1/62_000000000045_keypoints.npy\n",
            "Saved NPY file: /content/drive/My Drive/Surfing with Pose Estimation/rhGeneral/example62//output1/62_000000000040_keypoints.npy\n",
            "Saved NPY file: /content/drive/My Drive/Surfing with Pose Estimation/rhGeneral/example62//output1/62_000000000042_keypoints.npy\n",
            "Saved NPY file: /content/drive/My Drive/Surfing with Pose Estimation/rhGeneral/example62//output1/62_000000000041_keypoints.npy\n",
            "Saved NPY file: /content/drive/My Drive/Surfing with Pose Estimation/rhGeneral/example62//output1/62_000000000039_keypoints.npy\n",
            "Saved NPY file: /content/drive/My Drive/Surfing with Pose Estimation/rhGeneral/example62//output1/62_000000000038_keypoints.npy\n",
            "Saved NPY file: /content/drive/My Drive/Surfing with Pose Estimation/rhGeneral/example62//output1/62_000000000043_keypoints.npy\n",
            "Saved NPY file: /content/drive/My Drive/Surfing with Pose Estimation/rhGeneral/example62//output1/62_000000000044_keypoints.npy\n",
            "Saved NPY file: /content/drive/My Drive/Surfing with Pose Estimation/rhGeneral/example62//output1/62_000000000050_keypoints.npy\n",
            "Saved NPY file: /content/drive/My Drive/Surfing with Pose Estimation/rhGeneral/example62//output1/62_000000000016_keypoints.npy\n",
            "Saved NPY file: /content/drive/My Drive/Surfing with Pose Estimation/rhGeneral/example62//output1/62_000000000015_keypoints.npy\n",
            "Saved NPY file: /content/drive/My Drive/Surfing with Pose Estimation/rhGeneral/example62//output1/62_000000000014_keypoints.npy\n",
            "Saved NPY file: /content/drive/My Drive/Surfing with Pose Estimation/rhGeneral/example62//output1/62_000000000017_keypoints.npy\n",
            "Saved NPY file: /content/drive/My Drive/Surfing with Pose Estimation/rhGeneral/example62//output1/62_000000000032_keypoints.npy\n",
            "Saved NPY file: /content/drive/My Drive/Surfing with Pose Estimation/rhGeneral/example62//output1/62_000000000022_keypoints.npy\n",
            "Saved NPY file: /content/drive/My Drive/Surfing with Pose Estimation/rhGeneral/example62//output1/62_000000000035_keypoints.npy\n",
            "Saved NPY file: /content/drive/My Drive/Surfing with Pose Estimation/rhGeneral/example62//output1/62_000000000006_keypoints.npy\n",
            "Saved NPY file: /content/drive/My Drive/Surfing with Pose Estimation/rhGeneral/example62//output1/62_000000000000_keypoints.npy\n",
            "Saved NPY file: /content/drive/My Drive/Surfing with Pose Estimation/rhGeneral/example62//output1/62_000000000004_keypoints.npy\n",
            "Saved NPY file: /content/drive/My Drive/Surfing with Pose Estimation/rhGeneral/example62//output1/62_000000000005_keypoints.npy\n",
            "Saved NPY file: /content/drive/My Drive/Surfing with Pose Estimation/rhGeneral/example62//output1/62_000000000001_keypoints.npy\n",
            "Saved NPY file: /content/drive/My Drive/Surfing with Pose Estimation/rhGeneral/example62//output1/62_000000000003_keypoints.npy\n",
            "Saved NPY file: /content/drive/My Drive/Surfing with Pose Estimation/rhGeneral/example62//output1/62_000000000002_keypoints.npy\n",
            "Saved NPY file: /content/drive/My Drive/Surfing with Pose Estimation/rhGeneral/example62//output1/62_000000000009_keypoints.npy\n",
            "Saved NPY file: /content/drive/My Drive/Surfing with Pose Estimation/rhGeneral/example62//output1/62_000000000011_keypoints.npy\n",
            "Saved NPY file: /content/drive/My Drive/Surfing with Pose Estimation/rhGeneral/example62//output1/62_000000000007_keypoints.npy\n",
            "Saved NPY file: /content/drive/My Drive/Surfing with Pose Estimation/rhGeneral/example62//output1/62_000000000012_keypoints.npy\n",
            "Saved NPY file: /content/drive/My Drive/Surfing with Pose Estimation/rhGeneral/example62//output1/62_000000000008_keypoints.npy\n",
            "Saved NPY file: /content/drive/My Drive/Surfing with Pose Estimation/rhGeneral/example62//output1/62_000000000053_keypoints.npy\n",
            "Saved NPY file: /content/drive/My Drive/Surfing with Pose Estimation/rhGeneral/example62//output1/62_000000000069_keypoints.npy\n",
            "Saved NPY file: /content/drive/My Drive/Surfing with Pose Estimation/rhGeneral/example62//output1/62_000000000056_keypoints.npy\n",
            "Saved NPY file: /content/drive/My Drive/Surfing with Pose Estimation/rhGeneral/example62//output1/62_000000000067_keypoints.npy\n",
            "Saved NPY file: /content/drive/My Drive/Surfing with Pose Estimation/rhGeneral/example62//output1/62_000000000058_keypoints.npy\n",
            "Saved NPY file: /content/drive/My Drive/Surfing with Pose Estimation/rhGeneral/example62//output1/62_000000000062_keypoints.npy\n",
            "Saved NPY file: /content/drive/My Drive/Surfing with Pose Estimation/rhGeneral/example62//output1/62_000000000068_keypoints.npy\n",
            "Saved NPY file: /content/drive/My Drive/Surfing with Pose Estimation/rhGeneral/example62//output1/62_000000000051_keypoints.npy\n",
            "Saved NPY file: /content/drive/My Drive/Surfing with Pose Estimation/rhGeneral/example62//output1/62_000000000055_keypoints.npy\n",
            "Saved NPY file: /content/drive/My Drive/Surfing with Pose Estimation/rhGeneral/example62//output1/62_000000000059_keypoints.npy\n",
            "Saved NPY file: /content/drive/My Drive/Surfing with Pose Estimation/rhGeneral/example62//output1/62_000000000064_keypoints.npy\n",
            "Saved NPY file: /content/drive/My Drive/Surfing with Pose Estimation/rhGeneral/example62//output1/62_000000000052_keypoints.npy\n",
            "Saved NPY file: /content/drive/My Drive/Surfing with Pose Estimation/rhGeneral/example62//output1/62_000000000063_keypoints.npy\n",
            "Saved NPY file: /content/drive/My Drive/Surfing with Pose Estimation/rhGeneral/example62//output1/62_000000000065_keypoints.npy\n",
            "Saved NPY file: /content/drive/My Drive/Surfing with Pose Estimation/rhGeneral/example62//output1/62_000000000054_keypoints.npy\n",
            "Saved NPY file: /content/drive/My Drive/Surfing with Pose Estimation/rhGeneral/example62//output1/62_000000000061_keypoints.npy\n",
            "Saved NPY file: /content/drive/My Drive/Surfing with Pose Estimation/rhGeneral/example62//output1/62_000000000066_keypoints.npy\n",
            "Saved NPY file: /content/drive/My Drive/Surfing with Pose Estimation/rhGeneral/example62//output1/62_000000000074_keypoints.npy\n",
            "Saved NPY file: /content/drive/My Drive/Surfing with Pose Estimation/rhGeneral/example62//output1/62_000000000070_keypoints.npy\n",
            "Saved NPY file: /content/drive/My Drive/Surfing with Pose Estimation/rhGeneral/example62//output1/62_000000000071_keypoints.npy\n",
            "Saved NPY file: /content/drive/My Drive/Surfing with Pose Estimation/rhGeneral/example62//output1/62_000000000073_keypoints.npy\n",
            "Saved NPY file: /content/drive/My Drive/Surfing with Pose Estimation/rhGeneral/example62//output1/62_000000000072_keypoints.npy\n",
            "Saved NPY file: /content/drive/My Drive/Surfing with Pose Estimation/rhGeneral/example62//output1/62_000000000076_keypoints.npy\n",
            "Saved NPY file: /content/drive/My Drive/Surfing with Pose Estimation/rhGeneral/example62//output1/62_000000000075_keypoints.npy\n",
            "Saved first frame to /content/drive/My Drive/Surfing with Pose Estimation/rhGeneral/example62/62frame.jpg\n",
            "(667.0, 960.0)\n",
            "62_000000000062_keypoints.npy: 220.0\n",
            "62_000000000048_keypoints.npy: 220.0\n",
            "62_000000000049_keypoints.npy: 220.0\n",
            "62_000000000039_keypoints.npy: 220.0\n",
            "62_000000000002_keypoints.npy: 220.0\n",
            "62_000000000055_keypoints.npy: 220.00000000000023\n",
            "62_000000000074_keypoints.npy: 220.0\n",
            "62_000000000072_keypoints.npy: 219.9999999999999\n",
            "62_000000000044_keypoints.npy: 220.0\n",
            "62_000000000065_keypoints.npy: 219.9999999999999\n",
            "62_000000000014_keypoints.npy: 220.0\n",
            "62_000000000042_keypoints.npy: 220.0\n",
            "62_000000000066_keypoints.npy: 220.0\n",
            "62_000000000011_keypoints.npy: 220.0\n",
            "62_000000000068_keypoints.npy: 220.0\n",
            "62_000000000029_keypoints.npy: 219.9999999999999\n",
            "62_000000000064_keypoints.npy: 220.0\n",
            "62_000000000069_keypoints.npy: 220.0\n",
            "62_000000000038_keypoints.npy: 219.9999999999999\n",
            "62_000000000022_keypoints.npy: 220.0\n",
            "62_000000000006_keypoints.npy: 220.0\n",
            "62_000000000056_keypoints.npy: 220.0\n",
            "62_000000000020_keypoints.npy: 220.0\n",
            "62_000000000071_keypoints.npy: 220.0\n",
            "62_000000000041_keypoints.npy: 220.0\n",
            "62_000000000017_keypoints.npy: 220.0\n",
            "62_000000000000_keypoints.npy: 220.0\n",
            "62_000000000037_keypoints.npy: 219.9999999999999\n",
            "62_000000000057_keypoints.npy: 220.0\n",
            "62_000000000012_keypoints.npy: 219.9999999999999\n",
            "62_000000000003_keypoints.npy: 220.0\n",
            "62_000000000045_keypoints.npy: 220.0\n",
            "62_000000000076_keypoints.npy: 219.9999999999999\n",
            "62_000000000023_keypoints.npy: 219.9999999999999\n",
            "62_000000000061_keypoints.npy: 220.0\n",
            "62_000000000052_keypoints.npy: 220.0\n",
            "62_000000000043_keypoints.npy: 220.0\n",
            "62_000000000005_keypoints.npy: 220.0\n",
            "62_000000000009_keypoints.npy: 220.0\n",
            "62_000000000027_keypoints.npy: 220.0\n",
            "62_000000000070_keypoints.npy: 220.0\n",
            "62_000000000054_keypoints.npy: 220.0\n",
            "62_000000000016_keypoints.npy: 220.0\n",
            "62_000000000032_keypoints.npy: 220.0\n",
            "62_000000000035_keypoints.npy: 219.9999999999999\n",
            "62_000000000031_keypoints.npy: 220.0\n",
            "62_000000000025_keypoints.npy: 219.9999999999999\n",
            "62_000000000008_keypoints.npy: 220.0\n",
            "62_000000000004_keypoints.npy: 220.0\n",
            "62_000000000059_keypoints.npy: 220.0\n",
            "62_000000000051_keypoints.npy: 219.9999999999999\n",
            "62_000000000001_keypoints.npy: 220.0\n",
            "62_000000000063_keypoints.npy: 220.0\n",
            "62_000000000067_keypoints.npy: 220.0\n",
            "62_000000000050_keypoints.npy: 220.0000000000001\n",
            "62_000000000073_keypoints.npy: 220.0\n",
            "62_000000000028_keypoints.npy: 220.0\n",
            "62_000000000075_keypoints.npy: 220.0000000000001\n",
            "62_000000000058_keypoints.npy: 220.0\n",
            "62_000000000040_keypoints.npy: 220.0\n",
            "62_000000000015_keypoints.npy: 219.9999999999999\n",
            "62_000000000007_keypoints.npy: 220.0\n",
            "62_000000000060_keypoints.npy: 220.0\n",
            "62_000000000053_keypoints.npy: 220.0\n",
            "62_000000000030_keypoints.npy: 220.0000000000001\n",
            "plots printed to /content/drive/My Drive/Surfing with Pose Estimation/rhGeneral/example62//outputPlots\n",
            "Updated matrix saved to /content/drive/My Drive/Surfing with Pose Estimation/vertical_stacked_matrix.npy. Shape: (21282, 25)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZtG9pIJGRdJp"
      },
      "outputs": [],
      "source": [
        "#add threshhold to mitgate false positive keypoint detection"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# testing bed\n",
        "\n",
        "\n",
        "\n",
        "import os\n",
        "import json\n",
        "import numpy as np\n",
        "import cv2\n",
        "import os\n",
        "\n",
        "#before running script you need the jsons and the original video. place the json folder and the video within a folder in your google drive\n",
        "\n",
        "# 1. CREATE A FOLDER in google drive that contains the json folder file. also, put the original video in there too.\n",
        "# 2. DEFINE THESE VARIABLES BEFORE\n",
        "\n",
        "# parent folder name with a / at the end. for example, if named example, insert 'example/'. dont forget to add the sub-parent folder\n",
        "input_folder_name = 'Surfing with Pose Estimation/rhGeneral/example25/'\n",
        "\n",
        "#  folder with jsons and video.\n",
        "input_jsons = '25_jsons'\n",
        "input_video = '25.avi'\n",
        "\n",
        "# input the video name without .avi at the end. for example, if input_video is surfVid.avi, input_jpg will be surfVid\n",
        "input_jpg = '25'\n",
        "\n",
        "\n",
        "def json_to_npy_converter(input_folder, output_folder):\n",
        "    if not os.path.exists(output_folder):\n",
        "        os.makedirs(output_folder)\n",
        "\n",
        "    for file_name in os.listdir(input_folder):\n",
        "        if file_name.endswith(\".json\"):\n",
        "            input_file_path = os.path.join(input_folder, file_name)\n",
        "\n",
        "            with open(input_file_path, 'r') as file:\n",
        "                try:\n",
        "                    data = json.load(file)\n",
        "                    for person in data['people']:\n",
        "                        keypoints = person['pose_keypoints_2d']\n",
        "                        keypoints_array = np.array(keypoints).reshape(-1, 3)\n",
        "                        output_file_path = os.path.join(output_folder, file_name.replace('.json', '.npy'))\n",
        "                        np.save(output_file_path, keypoints_array)\n",
        "                        print(f\"Saved NPY file: {output_file_path}\")\n",
        "                except Exception as e:\n",
        "                    print(f\"Error processing file {input_file_path}: {e}\")\n",
        "\n",
        "# Replace the example paths with your actual paths\n",
        "input_folder = '/content/drive/My Drive/' + input_folder_name + input_jsons\n",
        "output_folder =  '/content/drive/My Drive/' + input_folder_name + '/output1'\n",
        "json_to_npy_converter(input_folder, output_folder)\n",
        "\n",
        "\n",
        "# video path extraction\n",
        "\n",
        "def extract_first_frame(video_path):\n",
        "    cap = cv2.VideoCapture(video_path)\n",
        "    if not cap.isOpened():\n",
        "        print(\"Error opening video file.\")\n",
        "        return\n",
        "\n",
        "    success, frame = cap.read()\n",
        "    if success:\n",
        "        output_filename = os.path.splitext(os.path.basename(video_path))[0] + \"frame.jpg\"\n",
        "        output_path = os.path.join(os.path.dirname(video_path), output_filename)\n",
        "\n",
        "        cv2.imwrite(output_path, frame)\n",
        "        print(f\"Saved first frame to {output_path}\")\n",
        "    else:\n",
        "        print(\"Failed to extract the first frame.\")\n",
        "\n",
        "    cap.release()\n",
        "#replace example path with actual path\n",
        "video_path = '/content/drive/My Drive/' + input_folder_name + input_video\n",
        "extract_first_frame(video_path)\n",
        "\n",
        "#finds frame center\n",
        "\n",
        "from PIL import Image\n",
        "\n",
        "def get_center_coordinates(image_path):\n",
        "    with Image.open(image_path) as img:\n",
        "        with Image.open(image_path):\n",
        "            height, width = img.size\n",
        "            center_x = width/2\n",
        "            center_y = height/2\n",
        "            return center_x,center_y\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "image_path = '/content/drive/My Drive/' + input_folder_name + input_jpg + 'frame.jpg'\n",
        "center_coordinates = get_center_coordinates(image_path)\n",
        "print(f\"{center_coordinates}\")\n",
        "\n",
        "#resize pose estimates to consistent size (height = 220 pixels)\n",
        "\n",
        "import numpy as np\n",
        "import os\n",
        "\n",
        "def rescale_pose_estimates(input_folder, output_folder, target_height=220):\n",
        "\n",
        "\n",
        "    if not os.path.exists(output_folder):\n",
        "        os.makedirs(output_folder)\n",
        "\n",
        "    for file_name in os.listdir(input_folder):\n",
        "        if file_name.endswith('.npy'):\n",
        "            file_path = os.path.join(input_folder, file_name)\n",
        "            pose_estimates = np.load(file_path)\n",
        "\n",
        "            # Calculate the current height\n",
        "            max_y = np.max(pose_estimates[:, 1])\n",
        "            min_y = np.min(pose_estimates[:, 1])\n",
        "            current_height = max_y - min_y\n",
        "\n",
        "\n",
        "            scaling_factor = target_height / current_height if current_height > 0 else 0\n",
        "\n",
        "\n",
        "            pose_estimates[:, :2] *= scaling_factor\n",
        "\n",
        "\n",
        "            new_file_path = os.path.join(output_folder, file_name)\n",
        "            np.save(new_file_path, pose_estimates)\n",
        "\n",
        "#replace the example paths with actual paths\n",
        "input_folder = output_folder\n",
        "output_folder = '/content/drive/My Drive/' + input_folder_name + '/output2'\n",
        "\n",
        "\n",
        "rescale_pose_estimates(input_folder, output_folder)\n",
        "\n",
        "#center keypoints in frame\n",
        "#use output from get_center_coordinates for \"frame_center\" variable (duh)\n",
        "#input folder is the ouput folder from rescale_pose_estimate\n",
        "\n",
        "import numpy as np\n",
        "import os\n",
        "\n",
        "def center_and_save_pose_estimates(input_folder, output_folder, frame_center):\n",
        "\n",
        "    if not os.path.exists(output_folder):\n",
        "        os.makedirs(output_folder)\n",
        "\n",
        "\n",
        "    for file_name in os.listdir(input_folder):\n",
        "        if file_name.endswith('.npy'):\n",
        "            file_path = os.path.join(input_folder, file_name)\n",
        "            keypoints = np.load(file_path)\n",
        "\n",
        "\n",
        "            center_of_mass = np.mean(keypoints[:, :2], axis=0)\n",
        "\n",
        "            difference = center_of_mass-frame_center\n",
        "\n",
        "            centered_keypoints = keypoints.copy()\n",
        "\n",
        "\n",
        "            centered_keypoints[:, :2] -= difference\n",
        "\n",
        "\n",
        "            output_path = os.path.join(output_folder, file_name)\n",
        "            np.save(output_path, centered_keypoints)\n",
        "\n",
        "\n",
        "input_folder = output_folder\n",
        "output_folder = '/content/gdrive/My Drive/' + input_folder_name + '/output3'\n",
        "frame_center = center_coordinates\n",
        "\n",
        "center_and_save_pose_estimates(input_folder, output_folder, frame_center)\n",
        "\"subtract difference between reference point and center point from each coordinate\"\n",
        "#check pose heights from each frame\n",
        "\n",
        "import numpy as np\n",
        "import os\n",
        "\n",
        "def calculate_pose_heights(directory):\n",
        "\n",
        "    heights = {}\n",
        "\n",
        "    for file_name in os.listdir(directory):\n",
        "        if file_name.endswith('.npy'):\n",
        "            file_path = os.path.join(directory, file_name)\n",
        "            keypoints = np.load(file_path)\n",
        "\n",
        "            height = np.max(keypoints[:, 1]) - np.min(keypoints[:, 1])\n",
        "            heights[file_name] = height\n",
        "\n",
        "    return heights\n",
        "\n",
        "#replace the example paths with actual paths\n",
        "directory =  output_folder\n",
        "\n",
        "\n",
        "pose_heights = calculate_pose_heights(directory)\n",
        "for file_name, height in pose_heights.items():\n",
        "    print(f\"{file_name}: {height}\")\n",
        "\n",
        "    #seperate pose estimates into arrays corresponding to keypoint ordering ^\n",
        "\n",
        "import numpy as np\n",
        "import os\n",
        "\n",
        "\n",
        "def bodypart_array_maker(input_folder, output_folder):\n",
        "\n",
        "    if not os.path.exists(output_folder):\n",
        "        os.makedirs(output_folder)\n",
        "\n",
        "    keypoints_accumulator = {} # a dictionary where the key refers to the keypoint index and the value is a list containing the corresponding keypoint accross each frame\n",
        "    #dictionary contains 25 indeces and each index contains a list that is 319 long\n",
        "    for file_name in os.listdir(input_folder):\n",
        "        if file_name.endswith('.npy'):\n",
        "            file_path = os.path.join(input_folder, file_name)\n",
        "            keypoints = np.load(file_path)\n",
        "\n",
        "            for i, keypoint in enumerate(keypoints):\n",
        "                if i not in keypoints_accumulator:\n",
        "                    keypoints_accumulator[i] = []   #creates list for each keypoint\n",
        "                keypoints_accumulator[i].append(keypoint) #populates each list one by one\n",
        "\n",
        "        for keypoint_index, keypoint_list in keypoints_accumulator.items():\n",
        "            output_filepath = os.path.join(output_folder, f'keypoint_{keypoint_index}.npy')\n",
        "            np.save(output_filepath, np.array(keypoint_list))\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "input_folder = output_folder\n",
        "output_folder = '/content/drive/My Drive/' + input_folder_name + '/output4'\n",
        "bodypart_array_maker(input_folder, output_folder)"
      ],
      "metadata": {
        "id": "FuqiHpyE_xVD",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 333
        },
        "outputId": "86cbc61f-4b6d-4eda-cc03-cff124434c5d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "Object arrays cannot be loaded when allow_pickle=False",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-18-4f0ed2abdbe1>\u001b[0m in \u001b[0;36m<cell line: 41>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     39\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m \u001b[0;31m# Call the function to stack all fat matrix rows into one big vertically stacked matrix\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 41\u001b[0;31m \u001b[0mupdate_vertical_matrix\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_folder\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput_file_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-18-4f0ed2abdbe1>\u001b[0m in \u001b[0;36mupdate_vertical_matrix\u001b[0;34m(input_folder, output_file_path)\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexists\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput_file_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m         \u001b[0;31m# If it exists, load the existing matrix\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m         \u001b[0mvertical_matrix\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput_file_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mallow_pickle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m         \u001b[0;31m# If not, initialize it as None (or an empty array)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/numpy/lib/npyio.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(file, mmap_mode, allow_pickle, fix_imports, encoding, max_header_size)\u001b[0m\n\u001b[1;32m    454\u001b[0m                                           max_header_size=max_header_size)\n\u001b[1;32m    455\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 456\u001b[0;31m                 return format.read_array(fid, allow_pickle=allow_pickle,\n\u001b[0m\u001b[1;32m    457\u001b[0m                                          \u001b[0mpickle_kwargs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpickle_kwargs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    458\u001b[0m                                          max_header_size=max_header_size)\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/numpy/lib/format.py\u001b[0m in \u001b[0;36mread_array\u001b[0;34m(fp, allow_pickle, pickle_kwargs, max_header_size)\u001b[0m\n\u001b[1;32m    793\u001b[0m         \u001b[0;31m# The array contained Python objects. We need to unpickle the data.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    794\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mallow_pickle\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 795\u001b[0;31m             raise ValueError(\"Object arrays cannot be loaded when \"\n\u001b[0m\u001b[1;32m    796\u001b[0m                              \"allow_pickle=False\")\n\u001b[1;32m    797\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mpickle_kwargs\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: Object arrays cannot be loaded when allow_pickle=False"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 207
        },
        "id": "xNejltnCFshJ",
        "outputId": "91ae80aa-701e-49e8-ca59-b39ee55f5d7e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'fat_matrix_row_file_path' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-19-c41788284535>\u001b[0m in \u001b[0;36m<cell line: 40>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     38\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m \u001b[0;31m# Specify the folder containing the fat matrix rows as .npy files\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 40\u001b[0;31m \u001b[0minput_folder\u001b[0m \u001b[0;34m=\u001b[0m  \u001b[0mfat_matrix_row_file_path\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     41\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m \u001b[0;31m# Specify the path of the existing (or to be created) vertically stacked big matrix\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'fat_matrix_row_file_path' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VWWoWTqMGX38",
        "outputId": "d344f118-355f-4356-e3f2-580c437c1cb8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Before update:\n",
            "Matrix file does not exist.\n",
            "\n",
            "After update:\n",
            "Matrix successfully loaded. Current shape: (1, 1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "HpJXFiCE-7EN",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 350
        },
        "outputId": "334aa85b-b5b2-4a08-fc28-e4baba11d68c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "Object arrays cannot be loaded when allow_pickle=False",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-12-c28a173990d6>\u001b[0m in \u001b[0;36m<cell line: 21>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;31m# Example usage\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0mnpy_file_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'/content/gdrive/My Drive/Surfing with Pose Estimation/vertical_stacked_matrix.npy'\u001b[0m \u001b[0;31m# Update this path to where your .npy matrix file is stored\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 21\u001b[0;31m \u001b[0mshow_matrix\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnpy_file_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-12-c28a173990d6>\u001b[0m in \u001b[0;36mshow_matrix\u001b[0;34m(npy_file_path)\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mshow_matrix\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnpy_file_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0;31m# Load the matrix from a .npy file\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m     \u001b[0mmatrix\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnpy_file_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0;31m# Set up the plot\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/numpy/lib/npyio.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(file, mmap_mode, allow_pickle, fix_imports, encoding, max_header_size)\u001b[0m\n\u001b[1;32m    454\u001b[0m                                           max_header_size=max_header_size)\n\u001b[1;32m    455\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 456\u001b[0;31m                 return format.read_array(fid, allow_pickle=allow_pickle,\n\u001b[0m\u001b[1;32m    457\u001b[0m                                          \u001b[0mpickle_kwargs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpickle_kwargs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    458\u001b[0m                                          max_header_size=max_header_size)\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/numpy/lib/format.py\u001b[0m in \u001b[0;36mread_array\u001b[0;34m(fp, allow_pickle, pickle_kwargs, max_header_size)\u001b[0m\n\u001b[1;32m    793\u001b[0m         \u001b[0;31m# The array contained Python objects. We need to unpickle the data.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    794\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mallow_pickle\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 795\u001b[0;31m             raise ValueError(\"Object arrays cannot be loaded when \"\n\u001b[0m\u001b[1;32m    796\u001b[0m                              \"allow_pickle=False\")\n\u001b[1;32m    797\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mpickle_kwargs\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: Object arrays cannot be loaded when allow_pickle=False"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}